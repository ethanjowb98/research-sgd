{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorBoard\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model, load_model\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msgd_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_csv, save_as_csv\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msgd_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Paths, Headers, Constants\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.src.models import Sequential\n",
    "from keras.src.losses.losses import CategoricalCrossentropy\n",
    "from keras.src.layers import Dense, TimeDistributed\n",
    "from keras.api.optimizers import SGD\n",
    "from keras.src.callbacks import TensorBoard\n",
    "from keras.api.models import save_model, load_model\n",
    "from ..sgd_functions import open_csv, save_as_csv\n",
    "from ..sgd_constants import Paths, Headers, Constants\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchSGD:\n",
    "    def __init__(self, learning_rate=0.01, clip_value=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_value = clip_value\n",
    "        self.optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    def train_step(self, model, features, labels, batch_size):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size=batch_size)\n",
    "\n",
    "        for batch_features, batch_labels in ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_features, training=True)   # Set training True for dropout etc.\n",
    "                loss = self.loss_function(batch_labels, predictions)\n",
    "\n",
    "            # Calculate gradients with gradient clipping\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            clipped_grads = [tf.clip_by_value(grad, -self.clip_value, self.clip_value) for grad in grads]\n",
    "\n",
    "            # Apply gradients with clipping\n",
    "            self.optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "\n",
    "    def loss_function(self, labels, predictions):\n",
    "        return CategoricalCrossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[1;32m      2\u001b[0m     TimeDistributed(Dense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m      3\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Output layer for predicting volume\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      6\u001b[0m ds: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m open_csv(Paths\u001b[38;5;241m.\u001b[39mfiltered_csv)\n\u001b[1;32m      8\u001b[0m dep \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVol\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# dependent var\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    TimeDistributed(Dense(16, activation=\"relu\")),\n",
    "    Dense(1)  # Output layer for predicting volume\n",
    "])\n",
    "\n",
    "ds: pd.DataFrame = open_csv(Paths.filtered_csv)\n",
    "\n",
    "dep = ds[\"Vol\"]  # dependent var\n",
    "ds.drop(columns=\"Vol\", inplace=True)  # independent var\n",
    "\n",
    "dep_max = dep.max()\n",
    "dep_min = dep.min()\n",
    "\n",
    "for c in Constants.categories:\n",
    "    ds[c] = ds[c].astype(\"category\", copy=False).cat.codes  # Adjusting ind var categories to int\n",
    "mms = MinMaxScaler()\n",
    "dep = mms.fit_transform(dep.values.reshape(-1, 1))  # Normalizing dep var from 0 to 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(ds, dep, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
